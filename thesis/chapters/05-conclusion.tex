\chapter{Conclusions}
\section{Conclusions}
One of the main goals of the thesis was to introduce and compare the performance of several machine learning methods concerning the weather forecasting task and to prove the hypothesis that these methods are promising in the context of future research. We believe that we have succeeded in this goal, in fact, we were even surprised by the quality of some solutions. Unexpectedly, even very naive methods, such as ridge regression, return satisfactory results. Another goal that we successfully achieved was the implementation of a model based on graph neural networks, which, proved to be very light and leverage a small number of parameters, which was not a part of our initial objective. Its results definitely beat other methods and approached the overall performance of an arbitrarily chosen topline, while beating it on features related to pressure and precipitation. This is yet another indication that MLWP methods will be crucial in future approaches to the weather forecasting. Analyses of the quality of solutions depending on the length of the input sequence and the predicted one have confirmed our hypotheses, and recently we have even managed to improve the predictive performance of the topline using a linear combination of its predictions with those returned by our graph architecture.

Limitations associated with the top-performing model, are related to its dissatisfactory predictive capabilities in terms of forecasting on features with challenging distributions, i.e. total precipitation and cloud cover.

\section{Future perspectives}
Future enhancements may include refining the graph architecture structure by integrating additional components previously mentioned in the literature review section. An illustrative approach may be the incorporation of fine-tuning with the autoregressive objective function, similar as was the case for FourCastNet (Section \ref{sec:fourcastnet}). 


Reconstructing our entire training pipeline so that the autoregressive optimization process would be performed at every training step also seems reasonable and worth analyzing. 


Detailed research in the area related to transformer-based architectures is also encouraging, as a suggestion we propose the usage of Vision Transformer as presented in MetNet to enrich our spatial context. Another idea might be to incorporate the ability of recurrent neural networks or attention mechanism to build even better temporal representation. 


It also seems an interesting idea to perform further experiments in the context of creating hybrid models from the solutions we have already implemented -- one might be to combine models with low correlation to form ensembles and see how such a solution would perform. 

Retraining and evaluating implemented solutions, while excluding features with challenging distributions, also seems natural. An interesting experiment would be to remove hard-to-predict features, such as total cloud cover and total precipitation, from the dataset. Since forecasting these variables is difficult, omitting them might yield better results for predicting other features.

In addition, in the case of gaining access to sufficient computing resources, we would be eager to test our architecture and the entire framework for analyzing its behavior for data from a period of more than one year and also from larger boundaries than those covering the administrative area of Poland. It is worth noting that increased computational power would also enable the prediction of additional features and extreme weather conditions.

It would be worth comparing the quality of predictions based on the prediction hour, not just the month. Finally, we believe training and evaluating the models using a more granular interval between timestamps, for example every 1 or 2 hours, would be an interesting analysis.
